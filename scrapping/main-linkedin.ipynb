{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# homemade functions\n",
    "from linkedin import *\n",
    "\n",
    "# selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# other\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, sec_sleep=0.5):\n",
    "        self.driver = webdriver.Firefox()\n",
    "        self.sec_sleep = sec_sleep\n",
    "        \n",
    "    def close_browser(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def scroll_to_bottom(self):\n",
    "        time.sleep(self.sec_sleep*10)\n",
    "        WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".jobs-search-results-list\")))\n",
    "        div_element = self.driver.find_element(By.CSS_SELECTOR, \".jobs-search-results-list\")\n",
    "        self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", div_element)\n",
    "        self.driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", div_element)\n",
    "\n",
    "    def load_page(self, url, try_quitting_first=True):\n",
    "        self.driver.get(url)\n",
    "        time.sleep(self.sec_sleep*2.5)\n",
    "\n",
    "    def accept_cookies(self):\n",
    "        cookies_xpath = '//*[@id=\"artdeco-global-alert-container\"]/div/section/div/div[2]/button[1]'\n",
    "        cookies_button = self.driver.find_element(By.XPATH, cookies_xpath)\n",
    "        cookies_button.click()\n",
    "        time.sleep(self.sec_sleep)\n",
    "\n",
    "    def login(self, email, password):\n",
    "        email_field = self.driver.find_element(By.XPATH, '//*[@id=\"session_key\"]')\n",
    "        email_field.send_keys(email)\n",
    "        time.sleep(self.sec_sleep)\n",
    "        \n",
    "        password_field = self.driver.find_element(By.XPATH, '//*[@id=\"session_password\"]')\n",
    "        password_field.send_keys(password)\n",
    "        time.sleep(self.sec_sleep)\n",
    "\n",
    "        button_signin = self.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section[1]/div/div/form/div[2]/button')\n",
    "        button_signin.click()\n",
    "        time.sleep(self.sec_sleep)\n",
    "\n",
    "    def press_enter_and_scroll(self):\n",
    "        search_field = self.driver.find_element(By.XPATH, \"(//*[contains(@id, 'jobs-search-box-location-id-ember')])[last()]\")\n",
    "        search_field.send_keys(Keys.ENTER)\n",
    "        time.sleep(self.sec_sleep)\n",
    "        self.scroll_to_bottom()\n",
    "\n",
    "    def close_message(self):\n",
    "        message_close = self.driver.find_element(By.XPATH, \"(//button[contains(@class, 'msg-overlay-bubble-header__control--new-convo-btn')])[last()]\")\n",
    "        message_close.click()\n",
    "        time.sleep(self.sec_sleep)\n",
    "\n",
    "    def enter_keywords(self, keywords):\n",
    "        search_field = self.driver.find_element(By.XPATH, \"(//*[contains(@id, 'jobs-search-box-keyword-id-ember')])[last()]\")\n",
    "        search_field.clear()\n",
    "        search_field.send_keys(keywords)\n",
    "        time.sleep(self.sec_sleep)\n",
    "\n",
    "    def enter_location(self, location):\n",
    "        search_field = self.driver.find_element(By.XPATH, \"(//*[contains(@id, 'jobs-search-box-location-id-ember')])[last()]\")\n",
    "        search_field.clear()\n",
    "        search_field.send_keys(location)\n",
    "        time.sleep(self.sec_sleep)\n",
    "\n",
    "    def get_job_details(self):\n",
    "            \n",
    "        # Find all job card elements\n",
    "        job_cards = driver.find_elements(By.CLASS_NAME, 'job-card-container')\n",
    "    \n",
    "        # Initialize lists to store job details\n",
    "        job_ids = []\n",
    "        job_titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        descriptions = []\n",
    "        posted_dates = []\n",
    "    \n",
    "        # Iterate over job card elements to extract details\n",
    "        for card in job_cards:\n",
    "            job_id = card.get_attribute('data-job-id')\n",
    "            title_element = card.find_element(By.CSS_SELECTOR, '.job-card-container__link.job-card-list__title')\n",
    "            company_element = card.find_element(By.CSS_SELECTOR, '.job-card-container__primary-description')\n",
    "            location_element = card.find_element(By.CSS_SELECTOR, '.job-card-container__metadata-wrapper li')\n",
    "            date_element = card.find_element(By.XPATH, \"//span[@class='tvm__text tvm__text--neutral']/span\")\n",
    "    \n",
    "            if job_id:\n",
    "                job_ids.append(job_id)\n",
    "                job_titles.append(title_element.text if title_element else 'N/A')\n",
    "                companies.append(company_element.text if company_element else 'N/A')\n",
    "                locations.append(location_element.text if location_element else 'N/A')\n",
    "                descriptions.append(card.text if card else 'N/A')\n",
    "                posted_dates.append(date_element.text if date_element else 'N/A')\n",
    "    \n",
    "        # Create a DataFrame\n",
    "        job_data = pd.DataFrame({\n",
    "            'Job ID': job_ids,\n",
    "            'Title': job_titles,\n",
    "            'Company': companies,\n",
    "            'Location': locations,\n",
    "            'Description': descriptions,\n",
    "            'Date': posted_dates,\n",
    "        })\n",
    "    \n",
    "        return job_data\n",
    "\n",
    "    def scrap_jobs(self, max_page=100, verbose=False):\n",
    "        # get job infos from first page\n",
    "        job_df = get_job_details(driver)\n",
    "        \n",
    "        # start with next page\n",
    "        page = 2\n",
    "        \n",
    "        while (True and page < max_page):\n",
    "        \n",
    "            # go to next page\n",
    "            next_page_xpath = f'//button[@aria-label=\"Page {page}\"]'\n",
    "            try:\n",
    "                next_page_button = driver.find_element(By.XPATH, next_page_xpath)\n",
    "                next_page_button.click()\n",
    "                page += 1\n",
    "            \n",
    "            # if next page not found, we stop scrapping and return the dataframe\n",
    "            except:\n",
    "                print(\"Last page, scrapping over\")\n",
    "                return job_df\n",
    "    \n",
    "            # wait and scroll down\n",
    "            scroll_to_bottom()\n",
    "    \n",
    "            # get job infos\n",
    "            new_job_df = get_job_details(driver)\n",
    "            \n",
    "            # print verbosity\n",
    "            if verbose:\n",
    "                    print(f\"Page {page} scrapped.\")\n",
    "                    print(f\"Jobs founded: {len(new_job_df)}\")\n",
    "            \n",
    "            job_df = pd.concat([new_job_df, job_df])\n",
    "            job_df = job_df.reset_index(drop=True)\n",
    "            job_df.to_csv('../data/job_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# credentials\n",
    "email = 'joseph.barbierdarnal@gmail.com'\n",
    "with open('../credentials.txt', 'r') as file:\n",
    "    password = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: # if a driver is already working, restart by quitting\n",
    "    scraper.close_br\n",
    "except:\n",
    "    pass\n",
    "\n",
    "scraper = Scraper()\n",
    "scraper.load_page(\"https://www.linkedin.com/\")\n",
    "scraper.accept_cookies()\n",
    "scraper.login(email, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last page, scrapping over\n"
     ]
    }
   ],
   "source": [
    "load_page(driver, \"https://www.linkedin.com/jobs/collections/recommended/\")\n",
    "close_message()\n",
    "\n",
    "enter_keywords(driver, 'data science')\n",
    "enter_location(driver, 'European Economic Area')\n",
    "press_enter_and_scroll()\n",
    "\n",
    "job_df = scrap_jobs(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Description</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3782287068</td>\n",
       "      <td>Business Intelligence Engineer II, S&amp;OP Automa...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Luxembourg, Luxembourg, Luxembourg</td>\n",
       "      <td>Business Intelligence Engineer II, S&amp;OP Automa...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3775375662</td>\n",
       "      <td>Internship Artificial Intelligence &amp; Computer ...</td>\n",
       "      <td>Korro AI</td>\n",
       "      <td>Darmstadt, Hesse, Germany (Hybrid)</td>\n",
       "      <td>Internship Artificial Intelligence &amp; Computer ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3782579723</td>\n",
       "      <td>Consultant Data Science (all genders)</td>\n",
       "      <td>Accenture DACH</td>\n",
       "      <td>Kronberg, Hesse, Germany (On-site)</td>\n",
       "      <td>Consultant Data Science (all genders)\\nAccentu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3781591727</td>\n",
       "      <td>Data Scientist for A320 Efficiency Leader (M/F)</td>\n",
       "      <td>Airbus</td>\n",
       "      <td>Toulouse, Occitanie, France (Hybrid)</td>\n",
       "      <td>Data Scientist for A320 Efficiency Leader (M/F...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3790751316</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>'s Heeren Loo</td>\n",
       "      <td>Amersfoort, Utrecht, Netherlands (On-site)</td>\n",
       "      <td>Data Engineer\\n's Heeren Loo\\nAmersfoort, Utre...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>3784864205</td>\n",
       "      <td>Senior Machine Learning Engineer</td>\n",
       "      <td>Ani Biome</td>\n",
       "      <td>Zagreb, Zagreb, Croatia (On-site)</td>\n",
       "      <td>Senior Machine Learning Engineer\\nAni Biome\\nZ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>3784559034</td>\n",
       "      <td>Lecturer in Data/Machine Learning Engineering ...</td>\n",
       "      <td>Breda University of Applied Sciences</td>\n",
       "      <td>Breda, North Brabant, Netherlands (On-site)</td>\n",
       "      <td>Lecturer in Data/Machine Learning Engineering ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>3651025688</td>\n",
       "      <td>Product Data Scientist Intern</td>\n",
       "      <td>Criteo</td>\n",
       "      <td>Paris, Île-de-France, France (Hybrid)</td>\n",
       "      <td>Product Data Scientist Intern\\nCriteo\\nParis, ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>3775702364</td>\n",
       "      <td>Senior Analyst</td>\n",
       "      <td>INVL Asset Management</td>\n",
       "      <td>Vilnius, Vilniaus, Lithuania (Hybrid)</td>\n",
       "      <td>Senior Analyst\\nINVL Asset Management\\nVilnius...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>3781836852</td>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Musti Group Nordic Oy</td>\n",
       "      <td>Helsinki Metropolitan Area (Hybrid)</td>\n",
       "      <td>Lead Data Engineer\\nMusti Group Nordic Oy\\nHel...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Job ID                                              Title  \\\n",
       "0    3782287068  Business Intelligence Engineer II, S&OP Automa...   \n",
       "1    3775375662  Internship Artificial Intelligence & Computer ...   \n",
       "2    3782579723              Consultant Data Science (all genders)   \n",
       "3    3781591727    Data Scientist for A320 Efficiency Leader (M/F)   \n",
       "4    3790751316                                      Data Engineer   \n",
       "..          ...                                                ...   \n",
       "279  3784864205                   Senior Machine Learning Engineer   \n",
       "280  3784559034  Lecturer in Data/Machine Learning Engineering ...   \n",
       "281  3651025688                      Product Data Scientist Intern   \n",
       "282  3775702364                                     Senior Analyst   \n",
       "283  3781836852                                 Lead Data Engineer   \n",
       "\n",
       "                                  Company  \\\n",
       "0                                  Amazon   \n",
       "1                                Korro AI   \n",
       "2                          Accenture DACH   \n",
       "3                                  Airbus   \n",
       "4                           's Heeren Loo   \n",
       "..                                    ...   \n",
       "279                             Ani Biome   \n",
       "280  Breda University of Applied Sciences   \n",
       "281                                Criteo   \n",
       "282                 INVL Asset Management   \n",
       "283                 Musti Group Nordic Oy   \n",
       "\n",
       "                                        Location  \\\n",
       "0             Luxembourg, Luxembourg, Luxembourg   \n",
       "1             Darmstadt, Hesse, Germany (Hybrid)   \n",
       "2             Kronberg, Hesse, Germany (On-site)   \n",
       "3           Toulouse, Occitanie, France (Hybrid)   \n",
       "4     Amersfoort, Utrecht, Netherlands (On-site)   \n",
       "..                                           ...   \n",
       "279            Zagreb, Zagreb, Croatia (On-site)   \n",
       "280  Breda, North Brabant, Netherlands (On-site)   \n",
       "281        Paris, Île-de-France, France (Hybrid)   \n",
       "282        Vilnius, Vilniaus, Lithuania (Hybrid)   \n",
       "283          Helsinki Metropolitan Area (Hybrid)   \n",
       "\n",
       "                                           Description         Date  \n",
       "0    Business Intelligence Engineer II, S&OP Automa...               \n",
       "1    Internship Artificial Intelligence & Computer ...               \n",
       "2    Consultant Data Science (all genders)\\nAccentu...               \n",
       "3    Data Scientist for A320 Efficiency Leader (M/F...               \n",
       "4    Data Engineer\\n's Heeren Loo\\nAmersfoort, Utre...               \n",
       "..                                                 ...          ...  \n",
       "279  Senior Machine Learning Engineer\\nAni Biome\\nZ...  2 weeks ago  \n",
       "280  Lecturer in Data/Machine Learning Engineering ...  2 weeks ago  \n",
       "281  Product Data Scientist Intern\\nCriteo\\nParis, ...  2 weeks ago  \n",
       "282  Senior Analyst\\nINVL Asset Management\\nVilnius...  2 weeks ago  \n",
       "283  Lead Data Engineer\\nMusti Group Nordic Oy\\nHel...  2 weeks ago  \n",
       "\n",
       "[284 rows x 6 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#job_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
